import streamlit as st
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
import nest_asyncio

# Ù„Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© event loop Ù…Ø¹ Streamlit
nest_asyncio.apply()

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
@st.cache_resource
def load_model():
    tokenizer = AutoTokenizer.from_pretrained("Elgeish/t5-small-arabic-qa")
    model = AutoModelForSeq2SeqLM.from_pretrained("Elgeish/t5-small-arabic-qa")
    return pipeline("text2text-generation", model=model, tokenizer=tokenizer)

def app():
    qa_pipeline = load_model()

    st.title("ğŸ’¬ Ø§Ø³Ø£Ù„ Ø¹Ù† Ø§Ù„Ù‚Ø±Ø¢Ù†")
    st.markdown("Ø£ÙƒØªØ¨ Ø£ÙŠ Ø³Ø¤Ø§Ù„ Ù„Ù‡ Ø¹Ù„Ø§Ù‚Ø© Ø¨Ø§Ù„Ù‚Ø±Ø¢Ù† Ø§Ù„ÙƒØ±ÙŠÙ… ÙˆØ³Ù†Ø­Ø§ÙˆÙ„ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„ÙŠÙ‡ âœ¨")

    question = st.text_input("â“ Ø³Ø¤Ø§Ù„Ùƒ:", placeholder="Ù…Ø«Ø§Ù„: ÙƒÙ… Ø¹Ø¯Ø¯ Ø¢ÙŠØ§Øª Ø³ÙˆØ±Ø© Ø§Ù„Ø¨Ù‚Ø±Ø©ØŸ")
    context = st.text_area("ğŸ“š Ø£Ø¶Ù Ø³ÙŠØ§Ù‚Ù‹Ø§ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ):", placeholder="Ù…Ø«Ø§Ù„: Ø³ÙˆØ±Ø© Ø§Ù„Ø¨Ù‚Ø±Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ...")

    if question:
        with st.spinner("â³ Ø¬Ø§Ø±ÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©..."):
            try:
                # ØµÙŠØºØ© T5 Ù„Ù„Ø³Ø¤Ø§Ù„: Ø³Ø¤Ø§Ù„: <Ø§Ù„Ø³Ø¤Ø§Ù„>  Ø³ÙŠØ§Ù‚: <Ø§Ù„Ø³ÙŠØ§Ù‚>
                input_text = f"Ø³Ø¤Ø§Ù„: {question}  Ø³ÙŠØ§Ù‚: {context}"
                result = qa_pipeline(input_text, max_new_tokens=50)[0]["generated_text"]
                st.success(f"âœ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: {result}")
            except Exception as e:
                st.error(f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„Ùƒ: {e}")

if __name__ == "__main__":
    app()
